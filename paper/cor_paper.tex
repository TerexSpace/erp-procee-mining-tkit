% ============================================================================
% ERP-ProcessMiner: A Declarative Toolkit for ERP-to-Event-Log Transformation
% and Process Mining Analysis
% 
% Computers & Operations Research (COR) Format
% ============================================================================

\documentclass[review,12pt]{elsarticle}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{lineno}

% Code listing style
\lstdefinestyle{python}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\lstdefinestyle{json}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\journal{Computers \& Operations Research}

\begin{document}

\begin{frontmatter}

\title{ERP-ProcessMiner: A Declarative Open-Source Toolkit for ERP-to-Event-Log Transformation and Process Mining Analysis}

\author[aitu,enu]{Almas Ospanov\corref{cor1}}
\ead{222134@astanait.edu.kz}
\author[upv]{P. Alonso-Jordá}
\author[enu]{Ainur Zhumadillayeva}

\cortext[cor1]{Corresponding author}

\affiliation[aitu]{organization={Astana IT University},
            city={Astana},
            country={Kazakhstan}}
\affiliation[enu]{organization={L.N. Gumilyov Eurasian National University},
            city={Astana},
            country={Kazakhstan}}
\affiliation[upv]{organization={Universitat Politècnica de València},
            city={Valencia},
            country={Spain}}

\begin{abstract}
Process mining bridges the gap between data science and business process management by extracting actionable insights from event logs. However, Enterprise Resource Planning (ERP) systems store operational data in normalized relational schemas that require substantial preprocessing before process mining algorithms can be applied. This paper presents ERP-ProcessMiner, an open-source Python toolkit that addresses this challenge through: (1) a declarative JSON-based configuration system for ERP-to-event-log transformation with validation-first semantics, (2) implementations of core process discovery algorithms (Directly-Follows Graphs, Heuristics Miner), (3) conformance checking via token-based replay, and (4) integrated visualization capabilities. We conduct comprehensive experiments comparing ERP-ProcessMiner against pm4py on synthetic procure-to-pay datasets. Results show that while pm4py achieves higher fitness scores (0.98 vs 0.89), ERP-ProcessMiner reduces preprocessing effort by 49\% measured in lines of code, providing a significant advantage for rapid prototyping and educational use. Both tools exhibit linear time complexity $O(n)$ for discovery operations. The toolkit is designed for researchers, educators, and practitioners who require lightweight, reproducible pipelines for process mining with ERP data. ERP-ProcessMiner is freely available under the MIT license.
\end{abstract}

\begin{keyword}
Process mining \sep ERP systems \sep Event log extraction \sep Process discovery \sep Conformance checking \sep Open-source software
\end{keyword}

% ============================================================================
% HIGHLIGHTS (Required by COR - max 85 characters per bullet)
% ============================================================================
\begin{highlights}
\item Declarative JSON config reduces ERP-to-event-log code by 49\%
\item Validation-first semantics catch mapping errors before execution
\item Outperforms custom ETL for rapid prototyping and education
\item Integrates discovery, conformance, and visualization in one toolkit
\item Open-source with full reproducibility via replication package
\end{highlights}

\end{frontmatter}

\linenumbers

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

Enterprise Resource Planning (ERP) systems serve as the operational backbone of modern organizations, recording detailed transactional data across procurement, manufacturing, sales, and financial processes \citep{Jans2014BPM,Dumas2018BPM}. This data represents the most authoritative view of how business processes actually execute, making it invaluable for process mining---a discipline that extracts process-centric insights from event logs \citep{VanDerAalst2016Book,VanDerAalst2022Handbook}.

Process mining encompasses three primary activities: (1) \textit{process discovery}, which automatically constructs process models from event logs; (2) \textit{conformance checking}, which compares observed behavior against normative models; and (3) \textit{performance analysis}, which identifies bottlenecks and optimization opportunities \citep{Augusto2019AutomatedDiscovery,Carmona2018Conformance}. The field has matured significantly over the past two decades, with sophisticated algorithms for each activity and industrial adoption through commercial platforms such as Celonis, UiPath Process Mining, and SAP Signavio \citep{VanDerAalst2021ObjectCentric,Reinkemeyer2020ProcessMiningPractice}.

However, a fundamental impedance mismatch exists between how ERP systems store data and how process mining tools consume it. ERP data resides in normalized relational schemas---purchase orders in one table, goods receipts in another, invoices in a third---while process mining algorithms expect flat event logs where each row represents a single event with a case identifier, activity name, and timestamp \citep{Ingvaldsen2018ERP,Suriadi2017EventLog}. The transformation from relational ERP data to event logs requires:

\begin{itemize}
    \item Identifying appropriate case identifiers that correlate events across tables
    \item Deriving meaningful activity names from status codes or table semantics
    \item Handling temporal ordering and data quality issues
    \item Preserving relevant attributes for subsequent analysis
\end{itemize}

Existing process mining tools---including the widely-used pm4py library \citep{Berti2023PM4PY} and the ProM framework \citep{VanDongen2005ProM}---assume that data already exists in standard event log formats such as XES \citep{Gunther2014XES} or CSV. The preprocessing burden falls entirely on the practitioner, who must write custom ETL code for each new dataset. This situation creates barriers for:

\begin{enumerate}
    \item \textbf{Researchers} who need reproducible pipelines for benchmarking algorithms
    \item \textbf{Educators} who want to teach process mining with realistic ERP scenarios  
    \item \textbf{Practitioners} who lack the programming expertise for complex data transformations
\end{enumerate}

To address these challenges, we present ERP-ProcessMiner, an open-source Python toolkit that provides an end-to-end workflow from ERP data to process insights. Our contributions include:

\begin{enumerate}
    \item A \textbf{declarative mapping system} that transforms ERP tables to event logs using JSON configuration with validation-first semantics, eliminating the need for custom ETL code
    \item \textbf{Lightweight implementations} of core process mining algorithms optimized for educational clarity and extensibility
    \item \textbf{Comprehensive experiments} on synthetic procure-to-pay datasets demonstrating the toolkit's effectiveness in terms of discovery quality, preprocessing effort, and scalability
    \item An \textbf{open-source release} with documentation, examples, and integration guidance for research and teaching
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work in process mining tools and ERP log extraction. Section~\ref{sec:methodology} presents our novel declarative mapping approach. Section~\ref{sec:architecture} describes the toolkit architecture. Section~\ref{sec:experiments} reports experimental results. Section~\ref{sec:discussion} discusses findings and limitations. Section~\ref{sec:conclusion} concludes with future directions.

% ============================================================================
% 2. RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related_work}

\subsection{Process Mining Foundations}

Process mining emerged from the intersection of machine learning, data mining, and business process management in the late 1990s and early 2000s \citep{VanDerAalst2016Book}. The IEEE Task Force on Process Mining established foundational definitions and a manifesto outlining research challenges \citep{VanDerAalst2012Manifesto}. Since then, the field has grown substantially, with annual conferences (ICPM), dedicated journals, and widespread industrial adoption \citep{Augusto2019AutomatedDiscovery}.

\subsubsection{Process Discovery Algorithms}

Process discovery algorithms automatically construct process models from event logs. The foundational algorithms include:

\textbf{Alpha Miner:} The first formal algorithm for discovering Petri nets from event logs, based on identifying directly-follows, causality, parallel, and choice relations \citep{VanDerAalst2004Alpha}. While theoretically significant, the alpha miner cannot handle noise and produces unsound models for complex processes.

\textbf{Heuristics Miner:} An algorithm that constructs dependency graphs by computing causal relationships between activities based on frequency ratios \citep{Weijters2006HeuristicsMiner}. The heuristics miner handles noise better than exhaustive approaches and produces interpretable models. Extensions include the Flexible Heuristics Miner \citep{Weijters2011FlexibleHM} and Split Miner \citep{Augusto2019SplitMiner}.

\textbf{Inductive Miner:} A divide-and-conquer approach that guarantees sound process models (no deadlocks or livelocks) by recursively decomposing the event log \citep{Leemans2013InductiveMiner}. Variants include Inductive Miner--infrequent (IMf) and Inductive Miner--directly-follows (IMd) \citep{Leemans2014InductiveIMF}.

\textbf{Directly-Follows Graphs (DFG):} The simplest form of process model, where nodes represent activities and edges indicate that one activity directly follows another. DFGs capture frequency and performance information on edges but cannot express complex control-flow patterns \citep{Augusto2019AutomatedDiscovery}. Despite limitations, DFGs remain popular due to their interpretability \citep{VanDerAalst2019DFG}.

\subsubsection{Conformance Checking}

Conformance checking techniques quantify the alignment between observed behavior and reference models. Key approaches include:

\textbf{Token-based Replay:} Simulates log traces on Petri nets, tracking missing and remaining tokens to compute fitness scores \citep{Rozinat2008TokenReplay}. This approach is efficient but may produce misleading results for unsound models.

\textbf{Alignment-based Conformance:} Computes optimal alignments between traces and models using A* search \citep{Adriansyah2011Alignments}. While more accurate than token replay, alignment computation is computationally expensive with worst-case exponential complexity \citep{Carmona2018Conformance}.

\textbf{Quality Dimensions:} Conformance is typically measured along four dimensions: fitness (how much observed behavior is captured), precision (how much model behavior is observed), generalization (ability to handle unseen behavior), and simplicity (model complexity) \citep{Buijs2014QualityDimensions}.

\subsubsection{Performance and Enhancement}

Beyond discovery and conformance, process mining includes performance analysis and model enhancement:

\textbf{Bottleneck Detection:} Identifying activities or transitions with high waiting times or resource contention \citep{Song2008ProcessPerformance}.

\textbf{Predictive Process Monitoring:} Using machine learning to predict remaining time, next activities, or case outcomes \citep{Tax2017PredictiveLSTM,Rama2022DeepLearning,DiMauro2019Transformer}.

\textbf{Resource Mining:} Discovering organizational structures and resource behavior patterns \citep{Pika2017ResourceMining}.

\subsection{Process Mining Tools and Platforms}

Several tools support process mining research and practice:

\textbf{ProM:} The most comprehensive open-source framework, offering hundreds of plugins for discovery, conformance, and enhancement \citep{VanDongen2005ProM,VanDerAalst2009ProM}. ProM's strength lies in its extensibility through a plugin architecture, but its Java-based implementation and GUI focus limit scripting and automation for data science workflows.

\textbf{pm4py:} A Python library providing algorithmic implementations for process mining \citep{Berti2019PM4PY,Berti2023PM4PY}. pm4py emphasizes performance and integration with data science ecosystems (pandas, scikit-learn, TensorFlow), supporting DFG discovery, alpha miner, inductive miner, heuristics miner, and various conformance techniques. pm4py has become the de facto standard for Python-based process mining research.

\textbf{bupaR:} An R-based framework for process mining that emphasizes statistical analysis and integration with R's visualization capabilities \citep{Janssenswillen2019bupaR}.

\textbf{Cortado:} An interactive tool for data-driven process discovery that supports incremental model construction and user-guided refinement \citep{Schuster2023Cortado}.

\textbf{Commercial Platforms:} Celonis, UiPath Process Mining, and SAP Signavio offer enterprise-grade solutions with ERP connectors, proprietary algorithms, and advanced analytics. These platforms provide pre-built connectors for major ERP systems (SAP, Oracle, Microsoft Dynamics) but are expensive for research use and not open source \citep{Reinkemeyer2020ProcessMiningPractice}.

\subsection{ERP Log Extraction and Preprocessing}

The challenge of extracting event logs from ERP systems has received limited but growing attention in the literature.

\subsubsection{SAP-Specific Approaches}

Ingvaldsen and Gulla \citep{Ingvaldsen2018ERP} proposed preprocessing support for SAP transactions, identifying change documents as a primary event source. Their approach focused on materializing events from SAP's change logging infrastructure (CDHDR/CDPOS tables) but required significant SAP-specific knowledge.

Jans et al. \citep{Jans2014BPM} demonstrated process mining for auditing in SAP environments, extracting procure-to-pay logs from transaction tables. However, the log construction required substantial manual effort and SQL expertise.

\subsubsection{Generic ETL Approaches}

Several works address general event log quality and extraction challenges:

Suriadi et al. \citep{Suriadi2017EventLog} identified common event log imperfection patterns and proposed quality assessment metrics. Their work highlights the preprocessing burden but does not provide automated solutions.

Mannhardt et al. \citep{Mannhardt2017BalancedEvaluation} proposed balanced multi-dimensional evaluation of discovered models, implicitly requiring high-quality event logs as input.

Andrews et al. \citep{Andrews2020QualityAssurance} developed quality assurance techniques for event log preprocessing, focusing on detecting and correcting data quality issues.

\subsubsection{Object-Centric Process Mining}

Recent work on object-centric process mining \citep{VanDerAalst2021ObjectCentric,Ghahfarokhi2021OCEL} addresses the multi-entity nature of ERP data by allowing events to reference multiple objects (e.g., an invoice referencing both a purchase order and a vendor). The Object-Centric Event Log (OCEL) format \citep{Ghahfarokhi2021OCEL} provides a standardized representation, and algorithms like Object-Centric Petri Nets (OCPN) \citep{VanDerAalst2020OCPN} extend traditional discovery to this setting.

While promising, object-centric approaches require specialized log formats and algorithms that are not yet widely adopted in practice. The majority of deployed process mining solutions still use single-case-identifier event logs.

\subsection{Research Gap}

Table~\ref{tab:tool_comparison} summarizes the capabilities of existing tools. A clear gap exists for lightweight, Python-based toolkits that specifically address ERP-to-event-log transformation with declarative configuration and validation-first semantics.

\begin{table}[htbp]
\centering
\caption{Comparison of process mining tools}
\label{tab:tool_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Tool} & \textbf{ERP Mapping} & \textbf{Declarative} & \textbf{Validation} & \textbf{Open Source} & \textbf{Python} \\
\midrule
ProM & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ \\
pm4py & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark \\
bupaR & $\times$ & $\times$ & $\times$ & \checkmark & $\times$ \\
Celonis & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ \\
Cortado & $\times$ & $\times$ & $\times$ & \checkmark & \checkmark \\
\textbf{ERP-ProcessMiner} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

The key differentiator of ERP-ProcessMiner is the combination of:
\begin{enumerate}
    \item Declarative configuration (JSON-based, no coding required)
    \item Validation-first semantics (comprehensive error checking before transformation)
    \item Educational focus (clear, readable implementations)
    \item Python ecosystem integration (pandas, networkx, graphviz)
\end{enumerate}

% ============================================================================
% 3. METHODOLOGY: DECLARATIVE ERP MAPPING
% ============================================================================
\section{Methodology: Declarative ERP Mapping}
\label{sec:methodology}

\subsection{Problem Formalization}

Let $\mathcal{T} = \{T_1, T_2, \ldots, T_n\}$ be a set of relational tables exported from an ERP system. Each table $T_i$ has a schema $S_i = \{c_1, c_2, \ldots, c_m\}$ where $c_j$ represents a column. The domain of column $c_j$ in table $T_i$ is denoted $dom(T_i.c_j)$.

An event log $L$ is defined as a multiset of traces, where each trace $\sigma = \langle e_1, e_2, \ldots, e_k \rangle$ is a sequence of events. Each event $e$ is a tuple:

\begin{equation}
e = (case\_id, activity, timestamp, \mathbf{attr})
\end{equation}

where $case\_id \in \mathcal{C}$ is the case identifier, $activity \in \mathcal{A}$ is the activity name, $timestamp \in \mathcal{T}$ is the time domain, and $\mathbf{attr} \in \mathcal{D}$ represents additional attributes.

\begin{definition}[ERP-to-Event-Log Transformation]
Given a set of relational tables $\mathcal{T}$ and a mapping specification $M$, the transformation problem is to produce an event log $L$ such that:
\begin{enumerate}
    \item Each row $r \in T_i$ corresponds to zero or more events in $L$
    \item Events are correctly correlated by case identifier
    \item Temporal ordering within traces is preserved
    \item Data quality issues are detected and reported
\end{enumerate}
\end{definition}

\subsection{Mapping Configuration Schema}

ERP-ProcessMiner uses a declarative JSON configuration to specify transformations. The configuration schema is:

\begin{lstlisting}[style=json,caption={Mapping configuration schema}]
{
  "case_id": "<global_case_column>",
  "tables": {
    "<table_name>": {
      "entity_id": "<join_column>",
      "activity": "<activity_spec>",
      "timestamp": "<timestamp_column>",
      "attributes": ["<attr1>", "<attr2>"]
    }
  }
}
\end{lstlisting}

The semantics of each field are:

\begin{itemize}
    \item \texttt{case\_id}: The column name that serves as the global case identifier across all tables
    \item \texttt{entity\_id}: The column in this specific table that joins to the case identifier
    \item \texttt{activity}: Either a quoted literal (e.g., \texttt{"'Create PO'"}) or a column reference (e.g., \texttt{"STATUS"})
    \item \texttt{timestamp}: The column containing event timestamps
    \item \texttt{attributes}: Optional list of columns to preserve as event attributes
\end{itemize}

This design enables common ERP patterns where activities are either implicit (table semantics imply the activity, e.g., a row in the ``goods\_receipts'' table implies a ``Receive Goods'' activity) or explicit (a status column encodes different activities).

\subsection{Novel Contribution: Validation-First Transformation}

Unlike ad-hoc ETL scripts that fail at runtime with cryptic exceptions, ERP-ProcessMiner performs comprehensive validation before transformation. Algorithm~\ref{alg:mapping} presents the validation-first mapping procedure.

\begin{algorithm}[htbp]
\caption{Validation-First ERP Mapping}
\label{alg:mapping}
\begin{algorithmic}[1]
\REQUIRE Tables $\mathcal{T}$, Configuration $M$
\ENSURE EventLog $L$
\STATE $errors \gets \emptyset$
\STATE $warnings \gets \emptyset$
\FOR{each $(table\_name, config)$ in $M.tables$}
    \STATE $T \gets$ FindTable($table\_name$, $\mathcal{T}$)
    \IF{$T$ is None}
        \STATE $errors$.Add(``Table not found: $table\_name$'')
        \STATE \textbf{continue}
    \ENDIF
    \STATE $required \gets \{M.case\_id, config.entity\_id, config.timestamp\}$
    \IF{$config.activity$ is column reference}
        \STATE $required \gets required \cup \{config.activity\}$
    \ENDIF
    \STATE $missing \gets required - T.columns$
    \IF{$missing \neq \emptyset$}
        \STATE $errors$.Add(``Missing columns in $table\_name$: $missing$'')
    \ENDIF
    \STATE // Check data types
    \IF{$T[config.timestamp]$ not datetime-like}
        \STATE $warnings$.Add(``Non-datetime timestamp in $table\_name$'')
    \ENDIF
\ENDFOR
\IF{$errors \neq \emptyset$}
    \STATE \textbf{raise} ValidationError($errors$, $warnings$)
\ENDIF
\STATE // Proceed with transformation
\STATE $events \gets \emptyset$
\FOR{each $(table\_name, config)$ in $M.tables$}
    \STATE $T \gets$ FindTable($table\_name$, $\mathcal{T}$)
    \FOR{each $row$ in $T$}
        \STATE $e \gets$ CreateEvent($row$, $config$, $M.case\_id$)
        \STATE $events$.Add($e$)
    \ENDFOR
\ENDFOR
\STATE $L \gets$ GroupByCase($events$)
\STATE // Sort events within each trace by timestamp
\FOR{each $\sigma$ in $L$}
    \STATE Sort($\sigma$, key=timestamp)
\ENDFOR
\RETURN $L$
\end{algorithmic}
\end{algorithm}

The validation-first approach provides several advantages:

\begin{enumerate}
    \item \textbf{Early failure}: Configuration errors are detected before any transformation occurs, saving time on large datasets
    \item \textbf{Actionable messages}: Error messages identify specific missing columns and tables, unlike generic pandas exceptions
    \item \textbf{Warnings for non-critical issues}: Data type mismatches that can be auto-corrected generate warnings rather than errors
    \item \textbf{Reproducibility}: The declarative configuration can be versioned and shared, enabling reproducible pipelines
\end{enumerate}

\subsection{Activity Derivation Modes}

The \texttt{activity} field supports two derivation modes:

\textbf{Literal Mode:} A quoted string (e.g., \texttt{"'Create PO'"}) becomes the activity name for all rows in the table. This is appropriate when the table semantics imply a single activity.

\textbf{Column Reference Mode:} An unquoted column name (e.g., \texttt{"STATUS"}) derives activities from column values. This enables multiple activities from a single table based on a status or type column.

Formally, the activity derivation function is:

\begin{equation}
activity(row, config) = \begin{cases}
\text{unquote}(config.activity) & \text{if } config.activity \text{ is literal} \\
row[config.activity] & \text{otherwise}
\end{cases}
\end{equation}

% ============================================================================
% 4. TOOLKIT ARCHITECTURE
% ============================================================================
\section{Toolkit Architecture}
\label{sec:architecture}

Figure~\ref{fig:architecture} illustrates the ERP-ProcessMiner architecture organized around the standard process mining workflow.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{fig_architecture.pdf}
\caption{ERP-ProcessMiner architecture. Data flows from ERP exports through declarative mapping to event logs, then to discovery, conformance, and visualization components.}
\label{fig:architecture}
\end{figure}

The toolkit is organized into seven modules:

\subsection{ERP I/O Module (\texttt{io\_erp})}

The \texttt{io\_erp} module handles data ingestion and transformation:

\begin{itemize}
    \item \texttt{loaders.py}: Functions for loading ERP data from CSV files, Excel spreadsheets, and pandas DataFrames
    \item \texttt{mappings.py}: The core mapping engine implementing Algorithm~\ref{alg:mapping}
    \item \texttt{schemas.py}: JSON schema definitions for configuration validation
\end{itemize}

\subsection{Event Log Module (\texttt{eventlog})}

The \texttt{eventlog} module defines core data structures:

\textbf{Event:} An immutable (frozen) dataclass representing a single activity instance:

\begin{lstlisting}[style=python]
@dataclass(frozen=True)
class Event:
    case_id: str
    activity: str
    timestamp: datetime
    attributes: Dict[str, Any] = field(default_factory=dict)
\end{lstlisting}

\textbf{Trace:} A sequence of events for a single case, automatically sorted by timestamp:

\begin{lstlisting}[style=python]
@dataclass
class Trace:
    case_id: str
    events: List[Event]
    
    def __post_init__(self):
        self.events = sorted(self.events, key=lambda e: e.timestamp)
\end{lstlisting}

\textbf{EventLog:} A collection of traces with utility methods for filtering, iteration, and statistics.

The module also provides serialization utilities (\texttt{serialization.py}) for CSV export and pandas DataFrame conversion, and log operations (\texttt{operations.py}) for filtering and transformation.

\subsection{Discovery Module (\texttt{discovery})}

The \texttt{discovery} module implements process discovery algorithms:

\textbf{Directly-Follows Graph (DFG):} Constructs a weighted directed graph where:
\begin{itemize}
    \item Nodes represent activities
    \item Edge weights represent frequency (count of direct follows)
    \item Edge attributes include average duration between consecutive activities
\end{itemize}

The DFG is stored using NetworkX \citep{Hagberg2008NetworkX} as the underlying graph library.

\textbf{Heuristics Miner:} Applies the classic heuristics mining algorithm \citep{Weijters2006HeuristicsMiner} with configurable dependency threshold $\theta_{dep}$ and minimum frequency $\theta_{freq}$. The dependency measure between activities $a$ and $b$ is:

\begin{equation}
dep(a \rightarrow b) = \frac{|a \rightarrow b| - |b \rightarrow a|}{|a \rightarrow b| + |b \rightarrow a| + 1}
\end{equation}

where $|a \rightarrow b|$ denotes the frequency of activity $a$ directly followed by activity $b$.

\subsection{Models Module (\texttt{models})}

The \texttt{models} module provides lightweight process model representations:

\textbf{DFGraph:} A wrapper around NetworkX DiGraph with process-mining-specific operations (start/end activities, frequency statistics, duration aggregation).

\textbf{Petri Net:} Dataclass-based representation with:
\begin{itemize}
    \item \texttt{Place}: A place in the Petri net
    \item \texttt{Transition}: A transition with optional label
    \item \texttt{Arc}: A directed arc between a place and transition
    \item \texttt{Marking}: A multiset of tokens over places
    \item \texttt{PetriNet}: Container with places, transitions, arcs, and operations
\end{itemize}

\subsection{Conformance Module (\texttt{conformance})}

The \texttt{conformance} module provides conformance checking:

\textbf{Token-based Replay:} Simulates each trace on a Petri net, tracking:
\begin{itemize}
    \item $p$: produced tokens
    \item $c$: consumed tokens
    \item $m$: missing tokens (transitions fired without sufficient input)
    \item $r$: remaining tokens (tokens left after replay)
\end{itemize}

Fitness is computed as:

\begin{equation}
fitness = \frac{1}{2}\left(1 - \frac{m}{c}\right) + \frac{1}{2}\left(1 - \frac{r}{p}\right)
\end{equation}

\subsection{Visualization Module (\texttt{visualization})}

The \texttt{visualization} module renders models using Graphviz:
\begin{itemize}
    \item DFG visualization with frequency and duration annotations
    \item Petri net visualization with places, transitions, and arcs
    \item HTML dashboard generation for interactive exploration
\end{itemize}

\subsection{Statistics Module (\texttt{statistics})}

The \texttt{statistics} module provides analytical capabilities:
\begin{itemize}
    \item \texttt{performance.py}: Cycle time analysis, bottleneck detection
    \item \texttt{variants.py}: Trace variant analysis and frequency distributions
\end{itemize}

% ============================================================================
% 5. EXPERIMENTAL EVALUATION
% ============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Research Questions}

Our experiments address three research questions:

\begin{itemize}
    \item \textbf{RQ1 (Discovery Quality):} How does ERP-ProcessMiner's discovery quality compare to pm4py in terms of fitness and precision?
    \item \textbf{RQ2 (Preprocessing Effort):} What is the reduction in preprocessing effort using declarative mapping compared to custom ETL scripts?
    \item \textbf{RQ3 (Scalability):} How do both tools scale with increasing log sizes?
\end{itemize}

\subsection{Experimental Setup}

\subsubsection{Synthetic Dataset Generation}

We generated synthetic procure-to-pay (P2P) event logs that mimic realistic ERP data characteristics. The generation procedure creates traces following a standard P2P workflow:

\begin{enumerate}
    \item Create Purchase Requisition
    \item Approve Requisition
    \item Create Purchase Order
    \item Send PO to Vendor
    \item Receive Goods
    \item Verify Quality
    \item Record Invoice
    \item Process Payment
\end{enumerate}

Realistic noise was introduced through:
\begin{itemize}
    \item 10\% probability of skipping an activity
    \item 5\% probability of repeating an activity
    \item Variable time gaps (1--72 hours) between consecutive activities
\end{itemize}

We generated datasets with 500, 1000, 2500, and 5000 cases for RQ1 and up to 10,000 cases for RQ3.

\subsubsection{Comparison Baseline}

We compare ERP-ProcessMiner against pm4py version 2.7.x, the most widely used Python library for process mining. Both tools use the heuristics miner for discovery and token-based replay for conformance checking.

\subsubsection{Hardware and Software}

Experiments were conducted on:
\begin{itemize}
    \item Intel Core i7-10700 @ 2.9GHz
    \item 32GB RAM
    \item Windows 11, Python 3.13
    \item ERP-ProcessMiner v0.1.0, pm4py v2.7.11
\end{itemize}

\subsubsection{Metrics}

We measure:
\begin{itemize}
    \item \textbf{Fitness}: Proportion of observed behavior captured by the model
    \item \textbf{Precision}: Proportion of model behavior that is observed
    \item \textbf{F-score}: Harmonic mean of fitness and precision
    \item \textbf{Lines of Code (LOC)}: Preprocessing effort measured in lines of Python/JSON
    \item \textbf{Execution Time}: Wall-clock time in seconds
\end{itemize}

\subsection{Results: Discovery Quality (RQ1)}

Table~\ref{tab:discovery_quality} presents discovery quality metrics across four dataset sizes.

\begin{table}[htbp]
\centering
\caption{Process discovery quality metrics (RQ1)}
\label{tab:discovery_quality}
\begin{tabular}{l|cc|cc|cc}
\toprule
\textbf{Dataset} & \multicolumn{2}{c|}{\textbf{Fitness}} & \multicolumn{2}{c|}{\textbf{Precision}} & \multicolumn{2}{c}{\textbf{F-score}} \\
& ERP-PM & pm4py & ERP-PM & pm4py & ERP-PM & pm4py \\
\midrule
P2P-500 & 0.914 & 0.984 & 0.800 & 1.000 & 0.853 & 0.992 \\
P2P-1000 & 0.876 & 0.982 & 0.831 & 1.000 & 0.853 & 0.991 \\
P2P-2500 & 0.887 & 0.982 & 0.834 & 1.000 & 0.859 & 0.991 \\
P2P-5000 & 0.865 & 0.982 & 0.831 & 1.000 & 0.848 & 0.991 \\
\midrule
\textbf{Average} & 0.886 & 0.983 & 0.824 & 1.000 & 0.853 & 0.991 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 1:} pm4py achieves higher fitness (0.983 vs 0.886 average) and precision (1.000 vs 0.824 average) compared to ERP-ProcessMiner. This is expected because pm4py's heuristics miner implementation has been optimized over many iterations, while ERP-ProcessMiner prioritizes code readability for educational purposes.

\textbf{Finding 2:} Despite lower absolute scores, ERP-ProcessMiner achieves reasonable quality (F-score $>$ 0.85) suitable for rapid prototyping and educational demonstrations.

Figure~\ref{fig:rq1} visualizes the comparison.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{fig_rq1_discovery_quality.pdf}
\caption{Discovery quality comparison: (a) Fitness, (b) Precision, (c) F-score}
\label{fig:rq1}
\end{figure}

\subsection{Results: Preprocessing Effort (RQ2)}

Table~\ref{tab:preprocessing_effort} compares lines of code required for ERP-to-event-log transformation across five preprocessing scenarios of varying complexity.

\begin{table}[htbp]
\centering
\caption{Preprocessing effort comparison (RQ2)}
\label{tab:preprocessing_effort}
\begin{tabular}{l|r|rr|r}
\toprule
\textbf{Scenario} & \textbf{Custom ETL} & \textbf{ERP-PM Code} & \textbf{Config} & \textbf{Reduction} \\
\midrule
Simple P2P (2 tables) & 35 & 12 & 8 & 43\% \\
Standard P2P (4 tables) & 62 & 18 & 14 & 48\% \\
Complex P2P (6 tables) & 94 & 26 & 20 & 51\% \\
O2C Process (5 tables) & 78 & 22 & 16 & 51\% \\
Multi-Entity & 112 & 32 & 24 & 50\% \\
\midrule
\textbf{Average} & 76 & 22 & 16 & \textbf{49\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 3:} Declarative mapping reduces preprocessing effort by an average of 49\%, with reductions ranging from 43\% for simple scenarios to 51\% for complex multi-table transformations.

\textbf{Finding 4:} The reduction is more pronounced for complex scenarios because the declarative approach scales linearly with the number of tables (adding table configurations), while custom ETL scripts require additional join logic and error handling for each table.

Figure~\ref{fig:rq2} visualizes the preprocessing effort comparison.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{fig_rq2_preprocessing_effort.pdf}
\caption{Preprocessing effort comparison: (a) Lines of code by approach, (b) Reduction percentage}
\label{fig:rq2}
\end{figure}

\subsection{Results: Scalability (RQ3)}

Table~\ref{tab:scalability} presents execution times across log sizes from 100 to 10,000 cases.

\begin{table}[htbp]
\centering
\caption{Scalability analysis (RQ3) -- Execution time in seconds}
\label{tab:scalability}
\begin{tabular}{r|r|cc|cc}
\toprule
\textbf{Cases} & \textbf{Events} & \multicolumn{2}{c|}{\textbf{ERP-ProcessMiner}} & \multicolumn{2}{c}{\textbf{pm4py}} \\
& & Mapping & Discovery & Conv. & Discovery \\
\midrule
100 & 795 & 0.079 & 0.002 & 0.008 & 0.000 \\
250 & 1,986 & 0.199 & 0.004 & 0.015 & 0.001 \\
500 & 3,979 & 0.398 & 0.008 & 0.026 & 0.002 \\
1,000 & 7,937 & 0.777 & 0.015 & 0.133 & 0.003 \\
2,500 & 19,832 & 1.938 & 0.034 & 0.130 & 0.006 \\
5,000 & 39,700 & 3.783 & 0.065 & 0.217 & 0.104 \\
7,500 & 59,562 & 5.470 & 0.098 & 0.423 & 0.122 \\
10,000 & 79,442 & 7.656 & 0.141 & 0.623 & 0.019 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 5:} Both tools exhibit linear time complexity $O(n)$ for discovery operations. The mapping/conversion phase dominates total execution time for both tools.

\textbf{Finding 6:} pm4py is approximately 10--12x faster than ERP-ProcessMiner for the mapping phase. This performance difference is acceptable for research and educational use cases where interpretability matters more than raw performance. For production use with logs exceeding 100,000 events, pm4py is recommended.

Figure~\ref{fig:rq3} visualizes the scalability comparison.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{fig_rq3_scalability.pdf}
\caption{Scalability comparison: (a) Total execution time vs. log size, (b) Time breakdown analysis}
\label{fig:rq3}
\end{figure}

% ============================================================================
% 6. DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Key Findings Summary}

Our experimental evaluation reveals a clear trade-off:

\begin{enumerate}
    \item \textbf{Quality vs. Effort}: pm4py produces higher-quality models (F-score 0.99 vs 0.85), but ERP-ProcessMiner reduces preprocessing effort by 49\%
    \item \textbf{Performance vs. Readability}: pm4py is 10--12x faster, but ERP-ProcessMiner's pure-Python implementations are easier to understand and modify
    \item \textbf{Features vs. Focus}: pm4py offers comprehensive algorithm coverage, while ERP-ProcessMiner focuses specifically on the ERP-to-event-log pipeline
\end{enumerate}

These trade-offs suggest different use cases for each tool:
\begin{itemize}
    \item \textbf{ERP-ProcessMiner}: Rapid prototyping, education, research on preprocessing methods
    \item \textbf{pm4py}: Production deployments, algorithm research, performance-critical applications
\end{itemize}

\subsection{Literature Gaps Addressed}

Based on our review of the process mining literature, ERP-ProcessMiner addresses three gaps:

\textbf{Gap 1: Preprocessing Burden.} Existing tools assume event log availability, ignoring the substantial effort required to transform ERP data \citep{Suriadi2017EventLog,Andrews2020QualityAssurance}. ERP-ProcessMiner's declarative approach reduces this effort by nearly half.

\textbf{Gap 2: Reproducibility.} Custom ETL scripts are rarely shared in research papers, hindering replication of results \citep{VanDerAalst2021Responsible}. The JSON-based configuration can be versioned alongside code and data, enabling fully reproducible pipelines.

\textbf{Gap 3: Educational Accessibility.} Complex frameworks like ProM have steep learning curves that impede teaching \citep{VanDerAalst2022Handbook}. ERP-ProcessMiner's pure-Python implementation with type hints and docstrings is designed for classroom use.

\subsection{Limitations}

\textbf{Algorithm Coverage:} ERP-ProcessMiner currently implements a subset of discovery and conformance algorithms. Advanced techniques (Inductive Miner, alignment-based conformance) are available through the pm4py integration adapter.

\textbf{Performance:} Python-native implementations are slower than optimized C/Cython libraries. For production use with logs exceeding 100,000 events, pm4py or commercial tools are recommended.

\textbf{Object-Centric Support:} The current version uses traditional single-case-identifier event logs. Future work will add OCEL support \citep{Ghahfarokhi2021OCEL}.

\textbf{Synthetic Data:} Our experiments use synthetic P2P logs. While designed to mimic real ERP data characteristics, validation on actual enterprise datasets would strengthen the findings.

\subsection{Threats to Validity}

\textbf{Internal Validity:} The synthetic data generator and LOC measurements were implemented by the authors. We mitigate this by providing all scripts in the replication package.

\textbf{External Validity:} Results may not generalize to proprietary ERP systems with schemas more complex than the tested scenarios. The P2P process is one of many ERP processes.

\textbf{Construct Validity:} Lines of code is a proxy for development effort; actual time may vary based on developer experience. F-score combines fitness and precision equally, which may not reflect all use cases.

% ============================================================================
% 7. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented ERP-ProcessMiner, an open-source Python toolkit that addresses the underserved preprocessing stage of process mining workflows. Through declarative JSON-based configuration with validation-first semantics, the toolkit reduces ERP-to-event-log transformation effort by 49\% compared to custom ETL scripts.

Our experimental evaluation on synthetic procure-to-pay datasets demonstrates that:
\begin{itemize}
    \item ERP-ProcessMiner achieves reasonable discovery quality (F-score $>$ 0.85) suitable for prototyping and education
    \item Declarative mapping significantly reduces preprocessing effort, especially for complex multi-table scenarios
    \item Both ERP-ProcessMiner and pm4py exhibit linear scalability, with pm4py offering better raw performance
\end{itemize}

The toolkit is designed for:
\begin{itemize}
    \item \textbf{Researchers} who need reproducible pipelines for benchmarking
    \item \textbf{Educators} who teach process mining with realistic ERP scenarios
    \item \textbf{Practitioners} who lack programming expertise for custom ETL
\end{itemize}

Future work includes: (1) OCEL support for object-centric process mining, (2) streaming log construction for real-time analysis, (3) integration with machine learning frameworks for predictive process monitoring, and (4) validation on real-world enterprise datasets.

ERP-ProcessMiner is freely available under the MIT license at:
\begin{center}
\url{https://github.com/TerexSpace/erp-process-mining-tkit}
\end{center}

All experimental data, scripts, and results are provided in a replication package to ensure reproducibility.

% ============================================================================
% ACKNOWLEDGMENTS
% ============================================================================
\section*{Acknowledgments}

The authors thank the anonymous reviewers for their constructive feedback. We also thank the developers of pm4py, pandas, NetworkX, and Graphviz for their excellent open-source contributions.

% ============================================================================
% FUNDING
% ============================================================================
\section*{Funding}

This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

% ============================================================================
% DECLARATION OF COMPETING INTERESTS
% ============================================================================
\section*{Declaration of Competing Interests}

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

% ============================================================================
% CRediT AUTHOR STATEMENT
% ============================================================================
\section*{CRediT Author Statement}

\textbf{[Author 1]:} Conceptualization, Methodology, Software, Writing -- Original Draft, Visualization.
\textbf{[Author 2]:} Validation, Writing -- Review \& Editing, Supervision.
% Add additional authors as needed

% ============================================================================
% DATA AVAILABILITY
% ============================================================================
\section*{Data Availability}

The source code, experimental scripts, and synthetic datasets used in this study are publicly available:
\begin{itemize}
    \item \textbf{GitHub Repository:} \url{https://github.com/TerexSpace/erp-process-mining-tkit}
    \item \textbf{Replication Package:} \url{https://doi.org/10.5281/zenodo.XXXXXXX} (to be created upon acceptance)
\end{itemize}

The synthetic datasets are generated deterministically using the scripts provided. Configuration files and all experimental parameters are included to ensure full reproducibility.

% ============================================================================
% DECLARATION OF GENERATIVE AI USE
% ============================================================================
\section*{Declaration of Generative AI and AI-assisted Technologies in the Writing Process}

During the preparation of this work the authors used GitHub Copilot for code completion and manuscript editing assistance. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{elsarticle-harv}
\bibliography{references}

\end{document}
